---
title: "Tutorial on Efficient LLM Inference"
collection: talks
type: "Tutorial"
permalink: /talks/2025-03-07-tutorial-1
venue: "INESC-ID"
date: 2025-03-07
location: "Lisbon, Portugal"
---

## Tutorial Overview

This tutorial covers essential techniques for efficient large language model (LLM) inference:

- **Infrastructure**: Selecting appropriate hardware and systems for inference workloads
- **Parallelism & Distributed Inference**: Scaling inference across multiple devices and nodes
- **Quantization**: Reducing model size and computational requirements through precision reduction
- **Serving Engines**: Deploying models with frameworks like vLLM for production environments
- **Batching**: Optimizing throughput by processing multiple requests simultaneously
- **Decoding Strategies**: Implementing efficient token generation methods
- **Streaming**: Real-time output delivery to clients
- **APIs**: Building interfaces for model access and integration
- **Caching**: Minimizing redundant computations through strategic memory management
